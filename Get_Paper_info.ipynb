{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Paper Info\n",
    "These notebook will get the paper info for each speakers in the conference. The notebook must be runned for the invited speaker table and the proceedings table afterward. Please note that running the notebook for the proceedings table will take serveral days to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "%run script/setup.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all files in Data folder as a datafrom, and add a column for the file name without the extension and folder name\n",
    "load_file = 'factInvited_init.csv' # or 'factProceedings_init.csv'\n",
    "df_initial  = pd.read_csv(os.path.join(filepath,load_file), encoding='utf-8')\n",
    "\n",
    "dblp_name = df_initial['Full name'] # or df_initial['Links'] if data is factProceedings_init.csv\n",
    "\n",
    "dblp_name = dblp_name.drop_duplicates().reset_index(drop=True)\n",
    "dblp_name = dblp_name.dropna().reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove disambiguation page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "remove_idx = []\n",
    "for disambiguation_page in tqdm(dblp_name):\n",
    "    i += 1\n",
    "    page = requests.get(disambiguation_page)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    if soup.find_all(\"div\", {\"class\": \"remark-box\"}):\n",
    "        remove_idx.append(i)\n",
    "    time.sleep(int(np.random.randint(1, 6, 1)))\n",
    "    \n",
    "for index in remove_idx:\n",
    "    del dblp_name[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from dblp.org\n",
    "Run the entire notebook for one dataframe at a time. First the invited speakers then the proceedings.\n",
    "https://dblp.org/search?q="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inivited speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dataframe\n",
    "df1 = pd.DataFrame(columns = ['Full name','Year','Year Count'])\n",
    "\n",
    "# This takes four days to run\n",
    "for Author in tqdm(dblp_name):\n",
    "    author_list = []\n",
    "    paper_list = []\n",
    "    \n",
    "    # Search for the author either by name\n",
    "    dblp_URL = \"https://dblp.org/search?q=\" + Author\n",
    "    \n",
    "    # Open the list of posible authors\n",
    "    page = requests.get(dblp_URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Check if there is page with the person's name\n",
    "    control_check = soup.find(id=\"completesearch-authors\").find_all(\"ul\",class_ = \"result-list\") \n",
    "    if len(control_check) > 0:\n",
    "        # Find the link of the first author\n",
    "        first_author = soup.find(id=\"completesearch-authors\").find_all(\"ul\",class_ = \"result-list\")[0].find_all(\"a\",href=True)[0]['href']\n",
    "        \n",
    "        # Enter the page about the chosen author\n",
    "        author_page = requests.get(first_author)\n",
    "        soup_of_speaker = BeautifulSoup(author_page.content, \"html.parser\")\n",
    "\n",
    "        Year_list = []\n",
    "\n",
    "        # Iterate through all sections and count the number of publications per year\n",
    "        sections = soup_of_speaker.find(id=\"publ-section\").find_all(\"div\", class_=\"hide-body\")\n",
    "        for s in range(0,len(sections)):\n",
    "            rows_in_sections = sections[s].find_all('ul', class_=\"publ-list\")[0].findAll(True, {\"class\":['year','entry inproceedings toc','entry article toc','entry incollection toc', 'entry book toc','entry editor toc','entry reference toc']})\n",
    "            last_row_idx = 0\n",
    "            \n",
    "            for row in range(0,len(rows_in_sections)):\n",
    "                #### Append year multipliers\n",
    "                if rows_in_sections[row].p == None:\n",
    "                    Year_list.append(rows_in_sections[row].text)\n",
    "                    paper_list.append(row-last_row_idx-1)\n",
    "                    last_row_idx = row \n",
    "                if row == len(rows_in_sections)-1: \n",
    "                    paper_list.append(len(rows_in_sections)-last_row_idx-1)\n",
    "            \n",
    "            # remove all -1 values from the list\n",
    "            paper_list = [x for x in paper_list if x >= 0]\n",
    "                \n",
    "        # Create a dataframe\n",
    "        df2 = pd.DataFrame(columns = ['Full name','Year','Year Count'])\n",
    "        # Append full name to the dataframe\n",
    "        \n",
    "        df2['Year'] = Year_list\n",
    "        df2['Year Count'] = paper_list\n",
    "        df2['Full name'] = Author\n",
    "        df1 = df1.append(df2, ignore_index=True)\n",
    "    time.sleep(int(np.random.randint(1, 6, 1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dataframe\n",
    "df1 = pd.DataFrame(columns = ['Full name','Year','Year Count'])\n",
    "\n",
    "# This takes four days to run\n",
    "for Author in tqdm(dblp_name):\n",
    "    author_list = []\n",
    "    paper_list = []\n",
    "    \n",
    "    # Search for the author either by name\n",
    "    dblp_URL = Author\n",
    "    \n",
    "    # Enter the page about the chosen author\n",
    "    author_page = requests.get(dblp_URL)\n",
    "    soup_of_speaker = BeautifulSoup(author_page.content, \"html.parser\")\n",
    "\n",
    "    Year_list = []\n",
    "\n",
    "    # Iterate through all sections and count the number of publications per year\n",
    "    sections = soup_of_speaker.find(id=\"publ-section\").find_all(\"div\", class_=\"hide-body\")\n",
    "    for s in range(0,len(sections)):\n",
    "        rows_in_sections = sections[s].find_all('ul', class_=\"publ-list\")[0].findAll(True, {\"class\":['year','entry inproceedings toc','entry article toc','entry incollection toc', 'entry book toc','entry editor toc','entry reference toc']})\n",
    "        last_row_idx = 0\n",
    "        \n",
    "        for row in range(0,len(rows_in_sections)):\n",
    "            #### Append year multipliers\n",
    "            if rows_in_sections[row].p == None:\n",
    "                Year_list.append(rows_in_sections[row].text)\n",
    "                paper_list.append(row-last_row_idx-1)\n",
    "                last_row_idx = row \n",
    "            if row == len(rows_in_sections)-1: \n",
    "                paper_list.append(len(rows_in_sections)-last_row_idx-1)\n",
    "        \n",
    "        # remove all -1 values from the list\n",
    "        paper_list = [x for x in paper_list if x >= 0]\n",
    "            \n",
    "    # Create a dataframe\n",
    "    df2 = pd.DataFrame(columns = ['Full name','Year','Year Count'])\n",
    "    # Append full name to the dataframe\n",
    "    \n",
    "    df2['Year'] = Year_list\n",
    "    df2['Year Count'] = paper_list\n",
    "    df2['Full name'] = Author\n",
    "    df1 = df1.append(df2, ignore_index=True)\n",
    "    time.sleep(int(np.random.randint(1, 6, 1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop_duplicates().reset_index(drop=True)\n",
    "df1['Year'] = df1['Year'].astype(int)\n",
    "df1['Year Count'] = df1['Year Count'].astype(int)\n",
    "\n",
    "df1 = df1.sort_values(by=['Year'], ascending=True)\n",
    "\n",
    "df1['Year Count'] = df1.groupby('Full name')['Year Count'].cumsum()\n",
    "\n",
    "df1['max_year_count'] = df1.groupby('Full name')['Year Count'].transform('max')\n",
    "df1['First year paper'] = df1.groupby('Full name')['Year'].transform('min')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data and save as factTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the paper count for a given name and year\n",
    "def get_paper_count(name, year):\n",
    "    df = df1[df1['Full name'] == name]\n",
    "    paper_count = df[df['Year'] <= year]['Year Count']\n",
    "    if len(paper_count) == 0:\n",
    "        return 0\n",
    "    return paper_count.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes and add a new column with the paper count\n",
    "merged = pd.merge(df_initial, df1.drop_duplicates(subset=['Full name']), on='Full name', how='left')\n",
    "merged['Year Count'] = merged.apply(lambda x: get_paper_count(x['Full name'], x['Year_x']), axis=1)\n",
    "\n",
    "# Fill empty rows with 0\n",
    "merged['max_year_count'] = merged['max_year_count'].fillna(0)\n",
    "merged['First year paper'] = merged['First year paper'].fillna(0)\n",
    "\n",
    "# Clean table\n",
    "merged = merged.drop(columns=['Year_y'])\n",
    "merged = merged.rename(columns={'Year_x': 'Year', 'Year Count': 'Paper Count', 'max_year_count': 'Max Paper Count'})\n",
    "merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"factInvited.csv\" # or factProceedings.csv\n",
    "merged.to_csv(os.path.join(filepath, filename), index=False)\n",
    "print(\"The file is now saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d92ff415db99feb7da45e8748a5c23d5d44d2038e70d5b05ab6a02856c817802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
